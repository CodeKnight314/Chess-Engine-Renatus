# Renatus

## Overview
This is a side project of mine to learn more about Deep-Q-Learning. The model, RenatusV2, is built on a convolutional network with residual blocks and convolutional layers, clocking in at 6.7 million trainable parameters. RenatusV2 is trained with combined paradigms, being first pretrained on supervised learning with games from FICS then undergoing self-play paradigm to discover new tactics. Renatus is also combined with herustics as part of the reward function to better guide decision making. Monte Carlos Tree Search is also used during choosing legal moves for better move exploration. Follow as I work and develop on this project over time! 
